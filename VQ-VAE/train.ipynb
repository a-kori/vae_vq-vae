{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# VQ-VAE Components\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv_trans = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_trans(x)\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        flat_x = x.view(-1, 512)  # Assuming the feature size from the encoder is 256\n",
    "        # Calculate distances between input and embedding vectors\n",
    "        distances = torch.cdist(flat_x, self.embedding.weight)\n",
    "        # Find the nearest embeddings\n",
    "        min_distances = distances.min(1, keepdim=True)[1]\n",
    "        # Quantize the input\n",
    "        quantized = self.embedding(min_distances).view_as(x)\n",
    "        return quantized\n",
    "\n",
    "# Select a fixed batch of images for visualization (5 images)\n",
    "fixed_images, _ = next(iter(DataLoader(dataset=train_dataset, batch_size=5, shuffle=True)))\n",
    "fixed_images = fixed_images.to(device)\n",
    "\n",
    "# Define the Encoder, Decoder, and VectorQuantizer classes (omitted for brevity)\n",
    "input_channels = 3\n",
    "# Model instantiation\n",
    "encoder = Encoder(input_channels).to(device)\n",
    "decoder = Decoder().to(device)\n",
    "vector_quantizer = VectorQuantizer(num_embeddings=100, embedding_dim=512).to(device)\n",
    "\n",
    "# Optimizer\n",
    "parameters = list(encoder.parameters()) + list(decoder.parameters()) + list(vector_quantizer.parameters())\n",
    "optimizer = optim.Adam(parameters, lr=0.001)\n",
    "\n",
    "# Function to plot images\n",
    "#def plot_images(images, title):\n",
    "#    images = images.cpu().detach().numpy()\n",
    "#    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "#    for idx, ax in enumerate(axes):\n",
    "#        ax.imshow(images[idx].reshape(32, 32), cmap='gray')\n",
    "#        ax.axis('off')\n",
    "#    plt.suptitle(title)\n",
    "#    plt.show()\n",
    "def plot_images(images, title):\n",
    "    images = images.cpu().detach().permute(0, 2, 3, 1).numpy()\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.imshow(images[idx], interpolation='nearest')\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoded = encoder(images)\n",
    "        quantized = vector_quantizer(encoded)\n",
    "        decoded = decoder(quantized)\n",
    "\n",
    "        loss = nn.functional.mse_loss(decoded, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # After each epoch, visualize the reconstruction of the fixed images\n",
    "    with torch.no_grad():\n",
    "        encoded = encoder(fixed_images)\n",
    "        quantized = vector_quantizer(encoded)\n",
    "        reconstructed_images = decoder(quantized)\n",
    "        plot_images(reconstructed_images, f'Reconstructed Images, Epoch {epoch+1}')\n",
    "\n",
    "print(\"Training complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
