{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, itertools\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),   \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv_trans = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid(),  # Output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_trans(x)\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / self.num_embeddings, 1 / self.num_embeddings)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z_flattened = z.view(-1, self.embedding_dim)\n",
    "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + torch.sum(self.embedding.weight**2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t())\n",
    "        encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
    "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
    "        \n",
    "        e_latent_loss = F.mse_loss(z_q.detach(), z)\n",
    "        q_latent_loss = F.mse_loss(z_q, z.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        z_q = z + (z_q - z).detach()\n",
    "        return loss, z_q, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, input_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(input_channels)\n",
    "        self.vector_quantizer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        vq_loss, quantized, _ = self.vector_quantizer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "\n",
    "fixed_images, _ = next(iter(DataLoader(dataset=train_dataset, batch_size=5, shuffle=True)))\n",
    "fixed_images = fixed_images.to(device)\n",
    "\n",
    "\n",
    "input_channels = 3\n",
    "commitment_cost = 0.25\n",
    "num_embeddings = 512\n",
    "embedding_dim = 512\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the VQ-VAE model\n",
    "vqvae_model = VQVAE(input_channels, num_embeddings, embedding_dim, commitment_cost).to(device)\n",
    "optimizer = optim.Adam(vqvae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def reconstruct_images(vqvae_model, data_loader, device, num_images=5):\n",
    "    vqvae_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    data_iter = iter(data_loader)\n",
    "    images, _ = next(data_iter)\n",
    "    images = images.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        decoded, _ = vqvae_model(images)  # Get the reconstructed images\n",
    "\n",
    "    # Select the first `num_images` images for display\n",
    "    images = images[:num_images].cpu()\n",
    "    decoded = decoded[:num_images].cpu()\n",
    "\n",
    "    # Plot original and reconstructed images\n",
    "    fig, axs = plt.subplots(2, num_images, figsize=(num_images * 2, 4))\n",
    "    for i in range(num_images):\n",
    "        axs[0, i].imshow(images[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        axs[0, i].axis('off')\n",
    "        axs[1, i].imshow(decoded[i].permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "        axs[1, i].axis('off')\n",
    "    axs[0, 0].set_title('Original')\n",
    "    axs[1, 0].set_title('Reconstructed')\n",
    "    plt.show()\n",
    "\n",
    "    vqvae_model.train()  # Set the model back to training mode\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        img, _ = data\n",
    "        img = img.to(device)\n",
    "\n",
    "        # Forward pass through the VQ-VAE model\n",
    "        decoded, vq_loss = vqvae_model(img)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        recon_loss = F.mse_loss(decoded, img)\n",
    "        total_loss = recon_loss + vq_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')\n",
    "\n",
    "    # Visualize reconstructed images after each epoch\n",
    "    reconstruct_images(vqvae_model, train_loader, device)\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
