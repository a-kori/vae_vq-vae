{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2R88GtApjf6O"
   },
   "source": [
    "# Fachprojekt Machine Learning\n",
    "Authors: Anastasiia Korzhylova, Ivan Shishkin, Ramneek Agnihotri, Rodi Mehi\n",
    "\n",
    "**Due date:** Wednesday, 1. May 2024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "cykyTX8rnQOk"
   },
   "source": [
    "## Import necessary components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR, CosineAnnealingLR\n",
    "\n",
    "# Import the training and the testing datasets\n",
    "from datasets import training_dataset, test_dataset\n",
    "\n",
    "# Import the evaluation and the sampling function\n",
    "from evaluation import evaluate\n",
    "from sampling import sample\n",
    "\n",
    "# Import the VAE model\n",
    "import networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "X3RVtvjjrCnI"
   },
   "source": [
    "## Set the hyperparameters, learning strategy, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 217955,
     "status": "ok",
     "timestamp": 1715068292983,
     "user": {
      "displayName": "Anastasiia Korzhylova",
      "userId": "09104997838746389444"
     },
     "user_tz": -120
    },
    "id": "Yz1B0uMXkZS5",
    "outputId": "110105e5-c2c8-4dce-e579-801f08c8f97e"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128 # Number of images per update of the network\n",
    "num_epochs = 50 # One epoch means seeing every image of the training dataset, which consists of 50000 images\n",
    "latent_dim = 100  # Size of the latent space\n",
    "input_channels = 3  # CIFAR-10 images have 3 color channels\n",
    "input_dim = 32 * 32 * input_channels  # Size of CIFAR-10 images (flattened)\n",
    "\n",
    "# Select the device that will be used for training: GPU, if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print('=========================================')\n",
    "\n",
    "# Put the neural network on the selected device\n",
    "model = networks.VAE(input_dim, latent_dim)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer selection\n",
    "optimizer_option = 'adam'\n",
    "\n",
    "optimizer = None\n",
    "if optimizer_option == \"adam\":\n",
    "  optimizer = torch.optim.Adam(model.parameters())\n",
    "elif optimizer_option == \"adamw\":\n",
    "  optimizer = torch.optim.AdamW(model.parameters())\n",
    "elif optimizer_option == \"rmsprop\":\n",
    "  optimizer = torch.optim.RMSProp(model.parameters())\n",
    "else:\n",
    "  optimizer = torch.optim.SGD(model.parameters())\n",
    "\n",
    "# Learning rate scheduler parameters\n",
    "lr_schedule_option = 'step'\n",
    "\n",
    "scheduler = None\n",
    "if lr_schedule_option == 'step':\n",
    "  scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "elif lr_schedule_option == 'exponential':\n",
    "  scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "elif lr_schedule_option == 'cosine':\n",
    "  scheduler = CosineAnnealingLR(optimizer, T_max=50, eta_min=0.0001)\n",
    "\n",
    "# Scaler for AMP\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create data loaders for training and testing with the batch size from above.\n",
    "# They can do things like multiprocessing and shuffling the order of the images.\n",
    "# We can iterate over them to obtain batches of images and labels (see training loop below).\n",
    "training_loader = DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q1FNGhYG_IbQ"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "  BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, input_dim), reduction='sum')\n",
    "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "  return BCE + KLD\n",
    "\n",
    "# We want to plot the accuracy at the end of training\n",
    "loss_curve = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  train_loss = 0\n",
    "  losses = []\n",
    "\n",
    "  for batch_idx, (data, _) in enumerate(training_loader):\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.cuda.amp.autocast():\n",
    "      # Forward pass\n",
    "      recon_batch, mu, logvar = model(data)\n",
    "      \n",
    "      # Compute loss\n",
    "      loss = loss_function(recon_batch, data, mu, logvar)\n",
    "\n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Optimization step\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    train_loss += loss.item()\n",
    "    \n",
    "  # Step the learning rate scheduler\n",
    "  scheduler.step()\n",
    "\n",
    "  # After the epoch, evaluate the accuracy on the test dataset\n",
    "  mean_loss = evaluate(model, test_loader, loss_function)\n",
    "  loss_curve.append(mean_loss)\n",
    "\n",
    "  print(f'Epoch {epoch + 1}, Loss: {mean_loss}, Learning Rate: {scheduler.get_last_lr()[0]:.6f}')\n",
    "  \n",
    "  # Generate and log images after each epoch\n",
    "  generated_images = sample(model)\n",
    "  for i, img in enumerate(generated_images):\n",
    "      plt.subplot(1, num_samples, i+1)\n",
    "      plt.imshow(img, cmap='gray')\n",
    "      plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot train and test losses\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot(train_losses, label='Train Loss')\n",
    "  plt.scatter(range(len(test_losses)), test_losses, color='red', label='Test Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.title('Train and Test Loss over Epochs')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot learning rate over time\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot(learning_rates, label='Learning Rate')\n",
    "  plt.xlabel('Batches')\n",
    "  plt.ylabel('Learning Rate')\n",
    "  plt.legend()\n",
    "  plt.title('Learning Rate over Time')\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "d25d8f77c9dea5a8c980ef8c2297485875df437f977b5aaad6473d2fbcee338b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
